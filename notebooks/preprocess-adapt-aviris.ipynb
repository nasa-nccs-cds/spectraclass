{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 13:52:40.003469: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "from spectraclass.data.base import DataManager\n",
    "from spectraclass.data.spatial.tile.manager import TileManager\n",
    "from spectraclass.data.spatial.modes import AvirisDataManager\n",
    "from typing import List, Union, Tuple, Optional, Dict, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we configure paths on the Jupyter server.  If these paths are not specified here then the default values,\n",
    "    defined in server-side config files, for the project (\"demo2\") and data mode (\"desis\"), will be used.  You can\n",
    "    choose whatever project names you want, they are used to save configurations and results for ongoing investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening log file:  '/home/tpmaxwel/.spectraclass/logging/aviris/img_mgr.log'\n",
      "Using config file: '/panfs/ccds02/home/tpmaxwel/JupyterLinks/spectraclass-dev/defaults/config.py'\n",
      "Using config file: '/home/tpmaxwel/.spectraclass/config/aviris/img_mgr.py'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/panfs/ccds02/app/modules/jupyter/ilab/tensorflow-kernel/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3194: UserWarning: Config option `use_model_data` not recognized by `DataManager`.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/panfs/ccds02/home/tpmaxwel/JupyterLinks/spectraclass-dev/notebooks/spectraclass/model/base.py:26: UserWarning: Config option `reduce_target_block` not recognized by `AvirisDataManager`.  Did you mean one of: `reduce_method, reduce_nblocks, reduce_nepoch`?\n",
      "  inst = cls(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dm: DataManager = DataManager.initialize( \"img_mgr\", 'aviris' )\n",
    "\n",
    "\n",
    "dm.modal.cache_dir = \"/explore/nobackup/projects/ilab/cache\"\n",
    "dm.modal.data_dir = \"/css/above/daac.ornl.gov/daacdata/above/ABoVE_Airborne_AVIRIS_NG/data/\"\n",
    "\n",
    "block_size = 150\n",
    "method = \"aec\" # \"vae\"\n",
    "model_dims = 32\n",
    "version = \"v2v2\"\n",
    "ts = \"20190801t153\"\n",
    "\n",
    "dm.modal.ext =  \"_img\"\n",
    "dm.proc_type = \"cpu\"\n",
    "dm.modal.images_glob = f\"ang{ts}*rfl/ang*_rfl_{version}/ang*_corr_{version}_img\"\n",
    "TileManager.block_size = block_size\n",
    "TileManager.reprocess = False\n",
    "AvirisDataManager.version = version\n",
    "dm.modal.valid_aviris_bands =  [ [5,193], [214,283], [319,10000] ]\n",
    "dm.modal.model_dims = model_dims\n",
    "dm.modal.reduce_method = method\n",
    "dm.modal.reduce_nepoch = 2\n",
    "dm.modal.reduce_focus_nepoch = 0\n",
    "dm.modal.reduce_niter = 12\n",
    "dm.modal.reduce_focus_ratio = 10.0\n",
    "dm.modal.reduce_dropout = 0.0\n",
    "dm.modal.reduce_learning_rate = 1e-4\n",
    "dm.modal.refresh_model = True\n",
    "dm.modal.reduce_nblocks = 1000\n",
    "dm.modal.reduce_nimages = 100\n",
    "dm.modal.modelkey = f\"b{block_size}.{method}.{ts}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is used to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preparing inputs, reprocess=False\n",
      "Completed Reading raster file /css/above/daac.ornl.gov/daacdata/above/ABoVE_Airborne_AVIRIS_NG/data//ang20190801t153402rfl/ang20190801t153402_rfl_v2v2/ang20190801t153402_corr_v2v2_img, dims = ('band', 'y', 'x'), shape = (425, 17861, 735)\n",
      "#Tile[0]-> Read Data: shape = (425, 17861, 735), dims=('band', 'y', 'x')\n",
      " Processing metadata for image ang20190801t153402rfl/ang20190801t153402_rfl_v2v2/ang20190801t153402_corr_v2v2_img with 600 blocks.\n",
      " ---> Writing metadata file at /explore/nobackup/projects/ilab/cache/spectraclass/aviris/img_mgr/b150.aec.20190801t153.mdata.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-26 14:31:59.434650: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 363)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 182)               66248     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 91)                16653     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 46)                4232      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                3008      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 363)               93291     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224,776\n",
      "Trainable params: 224,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 363)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 182)               66248     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 91)                16653     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 46)                4232      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                1504      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,637\n",
      "Trainable params: 88,637\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Autoencoder general training: 600 blocks for image[0/1]: ang20190801t153402rfl/ang20190801t153402_rfl_v2v2/ang20190801t153402_corr_v2v2_img\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 0), data shape = (8722, 363)\n",
      "Epoch 1/2\n",
      "35/35 [==============================] - 1s 5ms/step - loss: 0.9929\n",
      "Epoch 2/2\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.8431\n",
      " Trained autoencoder in 1.9974215030670166 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 1), data shape = (8722, 363)\n",
      "Epoch 3/4\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.5855\n",
      "Epoch 4/4\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.3863\n",
      " Trained autoencoder in 0.49873995780944824 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 2), data shape = (8722, 363)\n",
      "Epoch 5/6\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2919\n",
      "Epoch 6/6\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2702\n",
      " Trained autoencoder in 0.49908947944641113 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 3), data shape = (8722, 363)\n",
      "Epoch 7/8\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2657\n",
      "Epoch 8/8\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2604\n",
      " Trained autoencoder in 0.5081849098205566 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 4), data shape = (8722, 363)\n",
      "Epoch 9/10\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2512\n",
      "Epoch 10/10\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2315\n",
      " Trained autoencoder in 0.5100953578948975 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 5), data shape = (8722, 363)\n",
      "Epoch 11/12\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1896\n",
      "Epoch 12/12\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1391\n",
      " Trained autoencoder in 0.5222973823547363 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 6), data shape = (8722, 363)\n",
      "Epoch 13/14\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0976\n",
      "Epoch 14/14\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0690\n",
      " Trained autoencoder in 0.5109913349151611 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 7), data shape = (8722, 363)\n",
      "Epoch 15/16\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0527\n",
      "Epoch 16/16\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0451\n",
      " Trained autoencoder in 0.5063791275024414 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 8), data shape = (8722, 363)\n",
      "Epoch 17/18\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0423\n",
      "Epoch 18/18\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0420\n",
      " Trained autoencoder in 0.4982783794403076 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 9), data shape = (8722, 363)\n",
      "Epoch 19/20\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0413\n",
      "Epoch 20/20\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0411\n",
      " Trained autoencoder in 0.5023276805877686 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 10), data shape = (8722, 363)\n",
      "Epoch 21/22\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0407\n",
      "Epoch 22/22\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0405\n",
      " Trained autoencoder in 0.4981050491333008 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 11), data shape = (8722, 363)\n",
      "Epoch 23/24\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0403\n",
      "Epoch 24/24\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0402\n",
      " Trained autoencoder in 0.4996147155761719 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 12), data shape = (8722, 363)\n",
      "Epoch 25/26\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0402\n",
      "Epoch 26/26\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0397\n",
      " Trained autoencoder in 0.4876980781555176 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 13), data shape = (8722, 363)\n",
      "Epoch 27/28\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0398\n",
      "Epoch 28/28\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0394\n",
      " Trained autoencoder in 0.49007678031921387 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 14), data shape = (8722, 363)\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0394\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0391\n",
      " Trained autoencoder in 0.5111217498779297 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 15), data shape = (8722, 363)\n",
      "Epoch 31/32\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0392\n",
      "Epoch 32/32\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0391\n",
      " Trained autoencoder in 0.5011730194091797 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 16), data shape = (8722, 363)\n",
      "Epoch 33/34\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0387\n",
      "Epoch 34/34\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0387\n",
      " Trained autoencoder in 0.49761319160461426 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 17), data shape = (8722, 363)\n",
      "Epoch 35/36\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0385\n",
      "Epoch 36/36\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0384\n",
      " Trained autoencoder in 0.5061740875244141 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 18), data shape = (8722, 363)\n",
      "Epoch 37/38\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0383\n",
      "Epoch 38/38\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0383\n",
      " Trained autoencoder in 0.5082480907440186 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 19), data shape = (8722, 363)\n",
      "Epoch 39/40\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0382\n",
      "Epoch 40/40\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0381\n",
      " Trained autoencoder in 0.5174665451049805 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 20), data shape = (8722, 363)\n",
      "Epoch 41/42\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0379\n",
      "Epoch 42/42\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0378\n",
      " Trained autoencoder in 0.48052263259887695 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 21), data shape = (8722, 363)\n",
      "Epoch 43/44\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0379\n",
      "Epoch 44/44\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0377\n",
      " Trained autoencoder in 0.4779057502746582 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 22), data shape = (8722, 363)\n",
      "Epoch 45/46\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0374\n",
      "Epoch 46/46\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0374\n",
      " Trained autoencoder in 0.48066210746765137 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 23), data shape = (8722, 363)\n",
      "Epoch 47/48\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0371\n",
      "Epoch 48/48\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0368\n",
      " Trained autoencoder in 0.4848949909210205 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 24), data shape = (8722, 363)\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0365\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0362\n",
      " Trained autoencoder in 0.48583245277404785 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 25), data shape = (8722, 363)\n",
      "Epoch 51/52\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0362\n",
      "Epoch 52/52\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0359\n",
      " Trained autoencoder in 0.4992525577545166 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 26), data shape = (8722, 363)\n",
      "Epoch 53/54\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0359\n",
      "Epoch 54/54\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0358\n",
      " Trained autoencoder in 0.507941722869873 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 27), data shape = (8722, 363)\n",
      "Epoch 55/56\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0358\n",
      "Epoch 56/56\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0356\n",
      " Trained autoencoder in 0.5162904262542725 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 28), data shape = (8722, 363)\n",
      "Epoch 57/58\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0355\n",
      "Epoch 58/58\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0354\n",
      " Trained autoencoder in 0.4874100685119629 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 29), data shape = (8722, 363)\n",
      "Epoch 59/60\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0353\n",
      "Epoch 60/60\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0352\n",
      " Trained autoencoder in 0.5227727890014648 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 30), data shape = (8722, 363)\n",
      "Epoch 61/62\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0350\n",
      "Epoch 62/62\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0349\n",
      " Trained autoencoder in 0.5228593349456787 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 31), data shape = (8722, 363)\n",
      "Epoch 63/64\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0348\n",
      "Epoch 64/64\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0347\n",
      " Trained autoencoder in 0.5260136127471924 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 32), data shape = (8722, 363)\n",
      "Epoch 65/66\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0346\n",
      "Epoch 66/66\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.0344\n",
      " Trained autoencoder in 0.4919869899749756 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 33), data shape = (8722, 363)\n",
      "Epoch 67/68\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0344\n",
      "Epoch 68/68\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0342\n",
      " Trained autoencoder in 0.5162720680236816 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 34), data shape = (8722, 363)\n",
      "Epoch 69/70\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0342\n",
      "Epoch 70/70\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0340\n",
      " Trained autoencoder in 0.5290329456329346 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 35), data shape = (8722, 363)\n",
      "Epoch 71/72\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0339\n",
      "Epoch 72/72\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0338\n",
      " Trained autoencoder in 0.5262529850006104 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 36), data shape = (8722, 363)\n",
      "Epoch 73/74\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0338\n",
      "Epoch 74/74\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0336\n",
      " Trained autoencoder in 0.5344753265380859 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 37), data shape = (8722, 363)\n",
      "Epoch 75/76\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0335\n",
      "Epoch 76/76\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0334\n",
      " Trained autoencoder in 0.5216844081878662 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 38), data shape = (8722, 363)\n",
      "Epoch 77/78\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0334\n",
      "Epoch 78/78\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0332\n",
      " Trained autoencoder in 0.5277438163757324 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 39), data shape = (8722, 363)\n",
      "Epoch 79/80\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0331\n",
      "Epoch 80/80\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0331\n",
      " Trained autoencoder in 0.5319571495056152 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 40), data shape = (8722, 363)\n",
      "Epoch 81/82\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0329\n",
      "Epoch 82/82\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0328\n",
      " Trained autoencoder in 0.5224082469940186 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 41), data shape = (8722, 363)\n",
      "Epoch 83/84\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0327\n",
      "Epoch 84/84\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0326\n",
      " Trained autoencoder in 0.5388319492340088 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 42), data shape = (8722, 363)\n",
      "Epoch 85/86\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0325\n",
      "Epoch 86/86\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0322\n",
      " Trained autoencoder in 0.5287470817565918 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 43), data shape = (8722, 363)\n",
      "Epoch 87/88\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0321\n",
      "Epoch 88/88\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0320\n",
      " Trained autoencoder in 0.5253369808197021 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 44), data shape = (8722, 363)\n",
      "Epoch 89/90\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0319\n",
      "Epoch 90/90\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0317\n",
      " Trained autoencoder in 0.542396068572998 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 45), data shape = (8722, 363)\n",
      "Epoch 91/92\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0314\n",
      "Epoch 92/92\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0313\n",
      " Trained autoencoder in 0.5262622833251953 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 46), data shape = (8722, 363)\n",
      "Epoch 93/94\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0309\n",
      "Epoch 94/94\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0306\n",
      " Trained autoencoder in 0.5286264419555664 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 47), data shape = (8722, 363)\n",
      "Epoch 95/96\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0304\n",
      "Epoch 96/96\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0299\n",
      " Trained autoencoder in 0.5248537063598633 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 48), data shape = (8722, 363)\n",
      "Epoch 97/98\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0297\n",
      "Epoch 98/98\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0293\n",
      " Trained autoencoder in 0.5275566577911377 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 49), data shape = (8722, 363)\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0288\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0285\n",
      " Trained autoencoder in 0.53216552734375 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 50), data shape = (8722, 363)\n",
      "Epoch 101/102\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0281\n",
      "Epoch 102/102\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0278\n",
      " Trained autoencoder in 0.5239307880401611 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 51), data shape = (8722, 363)\n",
      "Epoch 103/104\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0273\n",
      "Epoch 104/104\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0268\n",
      " Trained autoencoder in 0.5243473052978516 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 52), data shape = (8722, 363)\n",
      "Epoch 105/106\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0263\n",
      "Epoch 106/106\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0260\n",
      " Trained autoencoder in 0.5217688083648682 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 53), data shape = (8722, 363)\n",
      "Epoch 107/108\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0256\n",
      "Epoch 108/108\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.0250\n",
      " Trained autoencoder in 0.5141308307647705 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 54), data shape = (16230, 363)\n",
      "Epoch 109/110\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0922\n",
      "Epoch 110/110\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0853\n",
      " Trained autoencoder in 0.7616355419158936 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 55), data shape = (16154, 363)\n",
      "Epoch 111/112\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0331\n",
      "Epoch 112/112\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0284\n",
      " Trained autoencoder in 0.7656352519989014 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 56), data shape = (15619, 363)\n",
      "Epoch 113/114\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.0483\n",
      "Epoch 114/114\n",
      "62/62 [==============================] - 0s 5ms/step - loss: 0.0461\n",
      " Trained autoencoder in 0.7487256526947021 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 57), data shape = (16383, 363)\n",
      "Epoch 115/116\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0343\n",
      "Epoch 116/116\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0259\n",
      " Trained autoencoder in 0.7641069889068604 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 58), data shape = (17075, 363)\n",
      "Epoch 117/118\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.0557\n",
      "Epoch 118/118\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.0393\n",
      " Trained autoencoder in 0.8316104412078857 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 59), data shape = (17497, 363)\n",
      "Epoch 119/120\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0316\n",
      "Epoch 120/120\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.0303\n",
      " Trained autoencoder in 0.8522396087646484 sec\n",
      " ** ITER[<built-in function iter>]: Processing block(0, 60), data shape = (17425, 363)\n",
      "Epoch 121/122\n",
      "22/69 [========>.....................] - ETA: 0s - loss: 0.0243"
     ]
    }
   ],
   "source": [
    "dm.prepare_inputs()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ILAB Kernel (TensorFlow)",
   "language": "python",
   "name": "tensorflow-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
